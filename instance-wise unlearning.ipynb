{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset, Dataset, random_split\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import torchvision.models as models\n",
        "import time\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import RandomSampler\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "import timm\n",
        "from collections import defaultdict\n",
        "from transformers import set_seed\n",
        "set_seed(123456)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "giIMj_g-LFY1"
      },
      "outputs": [],
      "source": [
        "# load CIFAR-10 dataset\n",
        "def load_cifar10_data(batch_size=256):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "\n",
        "    train_dataset = datasets.CIFAR10('./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.CIFAR10('./data', train=False, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# define resnet model\n",
        "class OriginalModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OriginalModel, self).__init__()\n",
        "        self.resnet18 = models.resnet18(pretrained=False)\n",
        "        self.resnet18.fc = nn.Linear(self.resnet18.fc.in_features, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet18(x)\n",
        "\n",
        "class UnlearnModel(OriginalModel):\n",
        "    pass\n",
        "\n",
        "# train model\n",
        "def train_model(model, train_loader, test_loader, epochs=40, max_lr=0.01, grad_clip=0.1, weight_decay=0, device='cuda'):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, steps_per_epoch=len(train_loader), epochs=epochs)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "            optimizer.step()\n",
        "            scheduler.step()  # update lr\n",
        "\n",
        "    return model\n",
        "\n",
        "@torch.no_grad()\n",
        "def actv_dist(model1, model2, dataloader, device = 'cuda'):\n",
        "    sftmx = nn.Softmax(dim = 1)\n",
        "    distances = []\n",
        "    for batch in dataloader:\n",
        "        x,_ = batch\n",
        "        x = x.to(device)\n",
        "        model1_out = model1(x)\n",
        "        model2_out = model2(x)\n",
        "        diff = torch.sqrt(torch.sum(torch.square(F.softmax(model1_out, dim = 1) - F.softmax(model2_out, dim = 1)), axis = 1))\n",
        "        diff = diff.detach().cpu()\n",
        "        distances.append(diff)\n",
        "    distances = torch.cat(distances, axis = 0)\n",
        "    return distances.mean()\n",
        "\n",
        "def evaluate(model, test_loader, device='cuda'):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    return 100. * correct / len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_single_forget_retain_loader(train_loader, size):\n",
        "    # get whole dataset\n",
        "    total_dataset = train_loader.dataset\n",
        "    \n",
        "    # random split dataset\n",
        "    forget_subset, retain_subset = random_split(total_dataset, [size, len(total_dataset) - size])\n",
        "\n",
        "    # create target dataLoader\n",
        "    forget_loader = DataLoader(forget_subset, batch_size=train_loader.batch_size, shuffle=True)\n",
        "    retain_loader = DataLoader(retain_subset, batch_size=train_loader.batch_size, shuffle=True)\n",
        "\n",
        "    return forget_loader, retain_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_instance_model_accuracy(model, test_loader, forget_loader, retain_loader, reference_loader, device):\n",
        "    model.to(device)\n",
        "    # Calculate accuracies on different datasets\n",
        "    test_accuracy = evaluate(model, test_loader, device)\n",
        "    forget_accuracy = evaluate(model, forget_loader, device)\n",
        "    retain_accuracy = evaluate(model, retain_loader, device)\n",
        "    reference_accuracy = evaluate(model, reference_loader, device)\n",
        "    \n",
        "    # Print out the accuracies\n",
        "    print(f\"Test Loader Accuracy: {test_accuracy:.2f}%\")\n",
        "    print(f\"Forget Loader Accuracy: {forget_accuracy:.2f}%\")\n",
        "    print(f\"Retain Loader Accuracy: {retain_accuracy:.2f}%\")\n",
        "    print(f\"Reference Loader Accuracy: {reference_accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def split_dataset(train_loader, reference_ratio=0.1):\n",
        "    class_indices = defaultdict(list)\n",
        "\n",
        "    for idx, (_, target) in enumerate(train_loader.dataset):\n",
        "        class_indices[target].append(idx)\n",
        "\n",
        "    reference_indices = []\n",
        "    train_indices = []\n",
        "\n",
        "    for indices in class_indices.values():\n",
        "        random.shuffle(indices)\n",
        "        k = max(1, int(len(indices) * reference_ratio))  \n",
        "        reference_indices.extend(indices[:k])\n",
        "        train_indices.extend(indices[k:])\n",
        "\n",
        "    reference_subset = Subset(train_loader.dataset, reference_indices)\n",
        "    train_subset = Subset(train_loader.dataset, train_indices)\n",
        "\n",
        "    reference_loader = DataLoader(reference_subset, batch_size=train_loader.batch_size, shuffle=True)\n",
        "    new_train_loader = DataLoader(train_subset, batch_size=train_loader.batch_size, shuffle=True)\n",
        "\n",
        "    return reference_loader, new_train_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   \n",
        "# load dataset and split dataset\n",
        "train_loader, test_loader = load_cifar10_data()\n",
        "reference_loader, new_train_loader = split_dataset(train_loader, reference_ratio=0.01)\n",
        "forget_loader64, retain_loader64 = create_single_forget_retain_loader(new_train_loader, size=64)\n",
        "forget_loader128, retain_loader128 = create_single_forget_retain_loader(new_train_loader, size=128)\n",
        "forget_loader256, retain_loader256 = create_single_forget_retain_loader(new_train_loader, size=256)\n",
        "forget_loader512, retain_loader512 = create_single_forget_retain_loader(new_train_loader,size=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# select instance to unlearn\n",
        "forget_loader = forget_loader64\n",
        "retain_loader = retain_loader64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_evaluate(model, criterion, optimizer, scheduler, epochs, device,train_loader=new_train_loader):\n",
        "    model.to(device)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for data, targets in train_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Evaluate performance and update learning rate\n",
        "        # Log the current learning rate\n",
        "        current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Current LR: {current_lr}')\n",
        "\n",
        "        # Evaluate performance and update learning rate\n",
        "        test_accuracy = evaluate(model, test_loader, device)\n",
        "        test_accuracy1 = evaluate(model, reference_loader, device)\n",
        "        scheduler.step(test_accuracy)\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Test Accuracy: {test_accuracy:.2f}%, Reference Accuracy: {test_accuracy1:.2f}%')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KkOtc-68UWep",
        "outputId": "af70563c-0897-4d3c-c00d-1d03f3bcfdd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40, Current LR: 0.1\n",
            "Epoch 1/40, Test Accuracy: 37.32%, Reference Accuracy: 36.80%\n",
            "Epoch 2/40, Current LR: 0.1\n",
            "Epoch 2/40, Test Accuracy: 61.62%, Reference Accuracy: 63.60%\n",
            "Epoch 3/40, Current LR: 0.1\n",
            "Epoch 3/40, Test Accuracy: 51.14%, Reference Accuracy: 54.80%\n",
            "Epoch 4/40, Current LR: 0.1\n",
            "Epoch 4/40, Test Accuracy: 67.56%, Reference Accuracy: 68.80%\n",
            "Epoch 5/40, Current LR: 0.1\n",
            "Epoch 5/40, Test Accuracy: 73.22%, Reference Accuracy: 74.00%\n",
            "Epoch 6/40, Current LR: 0.1\n",
            "Epoch 6/40, Test Accuracy: 74.62%, Reference Accuracy: 75.20%\n",
            "Epoch 7/40, Current LR: 0.1\n",
            "Epoch 7/40, Test Accuracy: 74.74%, Reference Accuracy: 78.20%\n",
            "Epoch 8/40, Current LR: 0.1\n",
            "Epoch 8/40, Test Accuracy: 80.12%, Reference Accuracy: 80.80%\n",
            "Epoch 9/40, Current LR: 0.1\n",
            "Epoch 9/40, Test Accuracy: 72.03%, Reference Accuracy: 72.20%\n",
            "Epoch 10/40, Current LR: 0.1\n",
            "Epoch 10/40, Test Accuracy: 78.75%, Reference Accuracy: 81.40%\n",
            "Epoch 11/40, Current LR: 0.1\n",
            "Epoch 11/40, Test Accuracy: 75.19%, Reference Accuracy: 74.60%\n",
            "Epoch 12/40, Current LR: 0.1\n",
            "Epoch 12/40, Test Accuracy: 74.48%, Reference Accuracy: 75.00%\n",
            "Epoch 13/40, Current LR: 0.010000000000000002\n",
            "Epoch 13/40, Test Accuracy: 85.93%, Reference Accuracy: 88.40%\n",
            "Epoch 14/40, Current LR: 0.010000000000000002\n",
            "Epoch 14/40, Test Accuracy: 86.12%, Reference Accuracy: 88.00%\n",
            "Epoch 15/40, Current LR: 0.010000000000000002\n",
            "Epoch 15/40, Test Accuracy: 86.21%, Reference Accuracy: 88.20%\n",
            "Epoch 16/40, Current LR: 0.010000000000000002\n",
            "Epoch 16/40, Test Accuracy: 86.24%, Reference Accuracy: 88.40%\n",
            "Epoch 17/40, Current LR: 0.010000000000000002\n",
            "Epoch 17/40, Test Accuracy: 86.33%, Reference Accuracy: 88.20%\n",
            "Epoch 18/40, Current LR: 0.010000000000000002\n",
            "Epoch 18/40, Test Accuracy: 86.39%, Reference Accuracy: 88.60%\n",
            "Epoch 19/40, Current LR: 0.010000000000000002\n",
            "Epoch 19/40, Test Accuracy: 86.44%, Reference Accuracy: 88.20%\n",
            "Epoch 20/40, Current LR: 0.010000000000000002\n",
            "Epoch 20/40, Test Accuracy: 86.37%, Reference Accuracy: 88.40%\n",
            "Epoch 21/40, Current LR: 0.010000000000000002\n",
            "Epoch 21/40, Test Accuracy: 86.62%, Reference Accuracy: 88.60%\n",
            "Epoch 22/40, Current LR: 0.010000000000000002\n",
            "Epoch 22/40, Test Accuracy: 86.32%, Reference Accuracy: 88.40%\n",
            "Epoch 23/40, Current LR: 0.010000000000000002\n",
            "Epoch 23/40, Test Accuracy: 86.34%, Reference Accuracy: 88.40%\n",
            "Epoch 24/40, Current LR: 0.010000000000000002\n",
            "Epoch 24/40, Test Accuracy: 86.39%, Reference Accuracy: 88.40%\n",
            "Epoch 25/40, Current LR: 0.010000000000000002\n",
            "Epoch 25/40, Test Accuracy: 86.43%, Reference Accuracy: 88.40%\n",
            "Epoch 26/40, Current LR: 0.0010000000000000002\n",
            "Epoch 26/40, Test Accuracy: 86.49%, Reference Accuracy: 88.60%\n",
            "Epoch 27/40, Current LR: 0.0010000000000000002\n",
            "Epoch 27/40, Test Accuracy: 86.40%, Reference Accuracy: 88.40%\n",
            "Epoch 28/40, Current LR: 0.0010000000000000002\n",
            "Epoch 28/40, Test Accuracy: 86.36%, Reference Accuracy: 89.00%\n",
            "Epoch 29/40, Current LR: 0.0010000000000000002\n",
            "Epoch 29/40, Test Accuracy: 86.41%, Reference Accuracy: 89.20%\n",
            "Epoch 30/40, Current LR: 0.00010000000000000003\n",
            "Epoch 30/40, Test Accuracy: 86.52%, Reference Accuracy: 88.80%\n",
            "Epoch 31/40, Current LR: 0.00010000000000000003\n",
            "Epoch 31/40, Test Accuracy: 86.48%, Reference Accuracy: 88.60%\n",
            "Epoch 32/40, Current LR: 0.00010000000000000003\n",
            "Epoch 32/40, Test Accuracy: 86.47%, Reference Accuracy: 88.40%\n",
            "Epoch 33/40, Current LR: 0.00010000000000000003\n",
            "Epoch 33/40, Test Accuracy: 86.48%, Reference Accuracy: 89.20%\n",
            "Epoch 34/40, Current LR: 1.0000000000000004e-05\n",
            "Epoch 34/40, Test Accuracy: 86.47%, Reference Accuracy: 88.40%\n",
            "Epoch 35/40, Current LR: 1.0000000000000004e-05\n",
            "Epoch 35/40, Test Accuracy: 86.45%, Reference Accuracy: 88.80%\n",
            "Epoch 36/40, Current LR: 1.0000000000000004e-05\n",
            "Epoch 36/40, Test Accuracy: 86.46%, Reference Accuracy: 88.40%\n",
            "Epoch 37/40, Current LR: 1.0000000000000004e-05\n",
            "Epoch 37/40, Test Accuracy: 86.47%, Reference Accuracy: 88.60%\n",
            "Epoch 38/40, Current LR: 1.0000000000000004e-06\n",
            "Epoch 38/40, Test Accuracy: 86.48%, Reference Accuracy: 89.20%\n",
            "Epoch 39/40, Current LR: 1.0000000000000004e-06\n",
            "Epoch 39/40, Test Accuracy: 86.49%, Reference Accuracy: 88.60%\n",
            "Epoch 40/40, Current LR: 1.0000000000000004e-06\n",
            "Epoch 40/40, Test Accuracy: 86.39%, Reference Accuracy: 88.80%\n"
          ]
        }
      ],
      "source": [
        "original_model = timm.create_model(\"resnet18\", pretrained=False)\n",
        "original_model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "original_model.maxpool = nn.Identity()  # type: ignore\n",
        "original_model.fc = nn.Linear(512,  10)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(original_model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0005, nesterov=True)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=3, threshold=0.001, mode='max')\n",
        "original_model = train_and_evaluate(original_model, criterion, optimizer, scheduler, epochs=40, device='cuda',train_loader=new_train_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loader Accuracy: 86.39%\n",
            "Forget Loader Accuracy: 100.00%\n",
            "Retain Loader Accuracy: 100.00%\n",
            "Reference Loader Accuracy: 88.80%\n"
          ]
        }
      ],
      "source": [
        "reference_acc = evaluate(original_model, reference_loader, device)\n",
        "evaluate_instance_model_accuracy(original_model, test_loader, forget_loader, retain_loader, reference_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def distill_with_soft_relabel(original_model, student_model, forget_loader, optimizer, epochs=1, device='cuda', distill_temperature=1):\n",
        "    original_model.to(device)\n",
        "    student_model.to(device)\n",
        "    epoch_losses = []  \n",
        "    epoch_times = []   \n",
        "    epsilon = '-inf'\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()  # Start timing the training (excluding evaluation)\n",
        "        student_model.train()\n",
        "        batch_losses = []\n",
        "\n",
        "        for batch_idx, (data, targets) in enumerate(forget_loader):\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = original_model(data)\n",
        "                # Find the index of maximum logit for each example and set it to -inf\n",
        "                max_indices = torch.argmax(teacher_logits, dim=1)\n",
        "                teacher_logits[torch.arange(teacher_logits.shape[0]), max_indices] = float(epsilon)\n",
        "                soft_labels = F.softmax(teacher_logits / distill_temperature, dim=1)\n",
        "            \n",
        "            student_logits = student_model(data)\n",
        "            student_log_probs = F.log_softmax(student_logits / distill_temperature, dim=1)\n",
        "            loss = F.kl_div(student_log_probs, soft_labels.detach(), reduction='batchmean')\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_losses.append(loss.item())\n",
        "\n",
        "        epoch_loss = sum(batch_losses) / len(batch_losses)\n",
        "        epoch_losses.append(epoch_loss)\n",
        "        \n",
        "        end_time = time.time()  # End timing the training\n",
        "        epoch_training_time = end_time - start_time\n",
        "        epoch_times.append(epoch_training_time)\n",
        "        \n",
        "        student_model.eval()  # Switch to evaluation mode for accuracy checking\n",
        "        accuracy = evaluate(student_model, forget_loader, device)\n",
        "        student_model.train()  # Switch back to training mode\n",
        "\n",
        "        # Optionally, evaluate model on forget_loader (if desired, add evaluation code here)\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss}, Training Time: {epoch_training_time:.2f} seconds')\n",
        "        \n",
        "        if accuracy <= reference_acc:\n",
        "            print(f\"Stopping early at epoch {epoch + 1} due to accuracy on forget_loader.\")\n",
        "            break\n",
        "\n",
        "\n",
        "    total_training_time = sum(epoch_times)\n",
        "    print(f\"Total training time: {total_training_time:.2f} seconds\")\n",
        "\n",
        "    return student_model, epoch_losses, total_training_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30, Loss: 8.621769905090332, Training Time: 0.26 seconds\n",
            "Epoch 2/30, Loss: 7.443272590637207, Training Time: 0.04 seconds\n",
            "Epoch 3/30, Loss: 6.221786022186279, Training Time: 0.04 seconds\n",
            "Epoch 4/30, Loss: 5.074117660522461, Training Time: 0.04 seconds\n",
            "Stopping early at epoch 4 due to accuracy on forget_loader.\n",
            "Total training time excluding evaluation: 0.37 seconds\n",
            "Test Loader Accuracy: 84.88%\n",
            "Forget Loader Accuracy: 68.75%\n",
            "Retain Loader Accuracy: 99.90%\n",
            "Reference Loader Accuracy: 88.60%\n"
          ]
        }
      ],
      "source": [
        "SU_model = timm.create_model(\"resnet18\", pretrained=False).to(device)\n",
        "SU_model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "SU_model.maxpool = nn.Identity() \n",
        "SU_model.fc = nn.Linear(512, 10)\n",
        "SU_model.load_state_dict(original_model.state_dict())\n",
        "SU_model.to(device)\n",
        "\n",
        "original_model.eval()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    SU_model.parameters(),\n",
        "    lr=0.00006,\n",
        ")\n",
        "SU_model, distillation_losses, total_training_time = distill_with_soft_relabel(original_model, SU_model, forget_loader, optimizer, epochs=30, device='cuda', distill_temperature=1)\n",
        "\n",
        "evaluate_instance_model_accuracy(SU_model, test_loader, forget_loader, retain_loader, reference_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SULI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "def calculate_entropy(logits):\n",
        "    probabilities = F.softmax(logits, dim=1)\n",
        "    log_probabilities = F.log_softmax(logits, dim=1)\n",
        "    entropy = -(probabilities * log_probabilities).sum(dim=1)\n",
        "    return entropy\n",
        "\n",
        "def sort_data(original_model, data_loader, device, batch_size_per_loader=200):\n",
        "    data_with_entropy = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, targets in data_loader:\n",
        "            data = data.to(device)\n",
        "            logits = original_model(data)\n",
        "            entropy = calculate_entropy(logits)\n",
        "            for i in range(len(data)):\n",
        "                data_with_entropy.append((data[i].cpu(), targets[i], entropy[i].item()))\n",
        "\n",
        "    # sort from high to low\n",
        "    data_with_entropy.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    # split dataset\n",
        "    loaders = []\n",
        "    for i in range(0, len(data_with_entropy), batch_size_per_loader):\n",
        "        batch = data_with_entropy[i:i+batch_size_per_loader]\n",
        "        dataset = [(x[0], x[1]) for x in batch]  \n",
        "        loader = DataLoader(dataset, batch_size=len(batch), shuffle=True)\n",
        "        loaders.append(loader)\n",
        "\n",
        "    return loaders\n",
        "\n",
        "def SPA_unlearning(original_model, student_model, forget_loader, optimizer, epochs=10, device='cuda', distill_temperature=1):\n",
        "    original_model.to(device)\n",
        "    student_model.to(device)\n",
        "    epoch_losses = []  \n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        student_model.train()\n",
        "        batch_losses = []\n",
        "\n",
        "        for batch_idx, (data, targets) in enumerate(forget_loader):\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = original_model(data)\n",
        "                max_indices = torch.argmax(teacher_logits, dim=1)\n",
        "                teacher_logits[torch.arange(teacher_logits.size(0)), max_indices] = float('-inf')\n",
        "                soft_labels = F.softmax(teacher_logits / distill_temperature, dim=1)\n",
        "            \n",
        "            student_logits = student_model(data)\n",
        "            student_log_probs = F.log_softmax(student_logits / distill_temperature, dim=1)\n",
        "            loss = F.kl_div(student_log_probs, soft_labels.detach(), reduction='batchmean')\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_losses.append(loss.item())\n",
        "\n",
        "        epoch_loss = sum(batch_losses) / len(batch_losses)\n",
        "        epoch_losses.append(epoch_loss)\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss}')\n",
        "\n",
        "    return student_model, epoch_losses\n",
        "\n",
        "def train_with_sorted_loaders(original_model, sorted_loaders, forget_loader, epochs=10, device='cuda', distill_temperature=1, lr=0.0005):\n",
        "    current_original_model = copy.deepcopy(original_model).to(device)\n",
        "    student_model = copy.deepcopy(current_original_model).to(device)\n",
        "    total_loss = 0  \n",
        "    total_loaders = len(sorted_loaders)\n",
        "    total_time_elapsed = 0  \n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        for loader_index, loader in enumerate(sorted_loaders):\n",
        "\n",
        "            optimizer = torch.optim.Adam(student_model.parameters(), lr=lr,)\n",
        "            start = time.time()  \n",
        "            student_model, epoch_losses = SPA_unlearning(current_original_model, student_model, loader, optimizer, epochs=1, device=device, distill_temperature=distill_temperature)\n",
        "            end = time.time()  \n",
        "            time_elapsed = end - start\n",
        "            total_time_elapsed += time_elapsed  \n",
        "            forget_test_accuracy = evaluate(student_model, forget_loader, device=device)\n",
        "\n",
        "            # print(f\"Loader {loader_index+1}/{total_loaders}, acc: {forget_test_accuracy}, Learning Rate: {lr}\")\n",
        "            \n",
        "            current_original_model = student_model\n",
        "            total_loss += sum(epoch_losses)  \n",
        "\n",
        "            if forget_test_accuracy <= reference_acc:  \n",
        "                # print(\"Early stopping triggered due to forget accuracy.\")\n",
        "                break\n",
        "        else:\n",
        "            continue\n",
        "        break\n",
        "    print(f\"Total time elapsed: {total_time_elapsed} seconds\")  \n",
        "\n",
        "    return current_original_model, total_loss, total_time_elapsed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, Loss: 6.582531929016113\n",
            "Epoch 1/1, Loss: 8.874137878417969\n",
            "Epoch 1/1, Loss: 3.642348527908325\n",
            "Total time elapsed over all epochs and loaders: 0.2001972198486328 seconds\n",
            "Runtime: 0.2001972198486328\n",
            "\n",
            "Test Loader Accuracy: 85.12%\n",
            "Forget Loader Accuracy: 85.94%\n",
            "Retain Loader Accuracy: 99.84%\n",
            "Reference Loader Accuracy: 88.20%\n"
          ]
        }
      ],
      "source": [
        "SULI_model = timm.create_model(\"resnet18\", pretrained=False).to(device)\n",
        "SULI_model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "SULI_model.maxpool = nn.Identity()  \n",
        "SULI_model.fc = nn.Linear(512, 10)\n",
        "SULI_model.load_state_dict(original_model.state_dict())\n",
        "SULI_model.to(device)\n",
        "\n",
        "sorted_loader = sort_data(original_model, forget_loader, device, batch_size_per_loader=32)\n",
        "SULI_model, total_loss, total_time_elapsed = train_with_sorted_loaders(SULI_model, sorted_loaders=sorted_loader, forget_loader=forget_loader, epochs=10 ,device='cuda',\n",
        "                                                                                 lr=0.00008)\n",
        "print(f\"Runtime: {total_time_elapsed}\\n\")\n",
        "evaluate_instance_model_accuracy(SULI_model, test_loader, forget_loader, retain_loader, reference_loader, device)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
