{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torchvision.models as models\n",
        "import time\n",
        "import copy\n",
        "import timm\n",
        "from transformers import set_seed\n",
        "set_seed(123456)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "giIMj_g-LFY1"
      },
      "outputs": [],
      "source": [
        "def load_cifar10_data(batch_size=256):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.CIFAR10('./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.CIFAR10('./data', train=False, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def split_dataset(train_dataset, target_forget):\n",
        "    labels = [train_dataset[i][1] for i in range(len(train_dataset))]\n",
        "\n",
        "    if not isinstance(target_forget, list):\n",
        "        target_forget = [target_forget]\n",
        "\n",
        "    forget_indices = [i for i, label in enumerate(labels) if label in target_forget]\n",
        "    retain_indices = [i for i, label in enumerate(labels) if label not in target_forget]\n",
        "\n",
        "    forget_data = Subset(train_dataset, forget_indices)\n",
        "    retain_data = Subset(train_dataset, retain_indices)\n",
        "\n",
        "    return forget_data, retain_data\n",
        "\n",
        "class TeacherModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TeacherModel, self).__init__()\n",
        "        self.resnet18 = models.resnet18(pretrained=False)\n",
        "        self.resnet18.fc = nn.Linear(self.resnet18.fc.in_features, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet18(x)\n",
        "\n",
        "class StudentModel(TeacherModel):\n",
        "    pass\n",
        "\n",
        "def create_timm_model():\n",
        "    # create ResNet18 \n",
        "    model = timm.create_model(\"resnet18\", pretrained=False)\n",
        "    model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "    model.maxpool = nn.Identity()\n",
        "    model.fc = nn.Linear(512, 10)\n",
        "    \n",
        "    return model\n",
        "\n",
        "# train_model\n",
        "def train_model(model, train_loader, test_loader, epochs=40, max_lr=0.01, grad_clip=0.1, weight_decay=0, device='cuda'):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, steps_per_epoch=len(train_loader), epochs=epochs)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "            optimizer.step()\n",
        "            scheduler.step()  # uodate learning rate\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to get class-wise accuracy\n",
        "def cifar10_class_wise_accuracy (model, data_loader, device='cuda'):\n",
        "    class_correct = [0 for i in range(10)]\n",
        "    class_total = [0 for i in range(10)]\n",
        "    class_names = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "    accuracies = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            c = (predicted == target).squeeze()\n",
        "            for i in range(target.size(0)):\n",
        "                label = target[i]\n",
        "                if c.numel() > 1:  # Handle batch size of 1\n",
        "                    class_correct[label] += c[i].item()\n",
        "                else:\n",
        "                    class_correct[label] += c.item()\n",
        "                class_total[label] += 1\n",
        "\n",
        "    for i in range(10):\n",
        "        accuracy = 100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0\n",
        "        accuracies[class_names[i]] = accuracy\n",
        "\n",
        "    return accuracies\n",
        "\n",
        "@torch.no_grad()\n",
        "def actv_dist(model1, model2, dataloader, device = 'cuda'):\n",
        "    sftmx = nn.Softmax(dim = 1)\n",
        "    distances = []\n",
        "    for batch in dataloader:\n",
        "        x,_ = batch\n",
        "        x = x.to(device)\n",
        "        model1_out = model1(x)\n",
        "        model2_out = model2(x)\n",
        "        diff = torch.sqrt(torch.sum(torch.square(F.softmax(model1_out, dim = 1) - F.softmax(model2_out, dim = 1)), axis = 1))\n",
        "        diff = diff.detach().cpu()\n",
        "        distances.append(diff)\n",
        "    distances = torch.cat(distances, axis = 0)\n",
        "    return distances.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, test_loader, device='cuda'):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    return 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "def evaluate_instance_model_accuracy(model, test_loader, forget_loader, retain_loader, device):\n",
        "    model.to(device)\n",
        "    # Calculate accuracies on different datasets\n",
        "    test_accuracy = evaluate(model, test_loader, device)\n",
        "    forget_accuracy = evaluate(model, forget_loader, device)\n",
        "    retain_accuracy = evaluate(model, retain_loader, device)\n",
        "    \n",
        "    # Print out the accuracies\n",
        "    print(f\"Test Loader Accuracy: {test_accuracy:.2f}%\")\n",
        "    print(f\"Forget Loader Accuracy: {forget_accuracy:.2f}%\")\n",
        "    print(f\"Retain Loader Accuracy: {retain_accuracy:.2f}%\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   \n",
        "# Load dataset and split dataset\n",
        "train_loader, test_loader = load_cifar10_data()\n",
        "# Select forgotten class\n",
        "forget_class_idxs=[1]\n",
        "forget_data, retain_data = split_dataset(train_loader.dataset, target_forget=forget_class_idxs)\n",
        "\n",
        "# Create DataLoader for forget_data\n",
        "forget_loader = DataLoader(forget_data, batch_size=256, shuffle=True)\n",
        "retain_loader = DataLoader(retain_data, batch_size=256, shuffle=True)\n",
        "\n",
        "forget_test_data, retain_test_data = split_dataset(test_loader.dataset, target_forget=forget_class_idxs)\n",
        "forget_test_loader = DataLoader(forget_test_data, batch_size=256, shuffle=True)\n",
        "retain_test_loader = DataLoader(retain_test_data, batch_size=256, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KkOtc-68UWep",
        "outputId": "af70563c-0897-4d3c-c00d-1d03f3bcfdd7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (act1): ReLU(inplace=True)\n",
              "  (maxpool): Identity()\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (drop_block): Identity()\n",
              "      (act1): ReLU(inplace=True)\n",
              "      (aa): Identity()\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act2): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (drop_block): Identity()\n",
              "      (act1): ReLU(inplace=True)\n",
              "      (aa): Identity()\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (drop_block): Identity()\n",
              "      (act1): ReLU(inplace=True)\n",
              "      (aa): Identity()\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act2): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (drop_block): Identity()\n",
              "      (act1): ReLU(inplace=True)\n",
              "      (aa): Identity()\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (drop_block): Identity()\n",
              "      (act1): ReLU(inplace=True)\n",
              "      (aa): Identity()\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act2): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (drop_block): Identity()\n",
              "      (act1): ReLU(inplace=True)\n",
              "      (aa): Identity()\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (drop_block): Identity()\n",
              "      (act1): ReLU(inplace=True)\n",
              "      (aa): Identity()\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act2): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (drop_block): Identity()\n",
              "      (act1): ReLU(inplace=True)\n",
              "      (aa): Identity()\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load original model\n",
        "original_model = create_timm_model().to(device)\n",
        "original_model.load_state_dict(\n",
        "            torch.hub.load_state_dict_from_url(\n",
        "                      \"https://huggingface.co/edadaltocg/resnet18_cifar10/resolve/main/pytorch_model.bin\",\n",
        "                       map_location=\"cpu\", \n",
        "                       file_name=\"resnet18_cifar10.pth\",\n",
        "             )\n",
        ")\n",
        "original_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loader Accuracy: 94.98%\n",
            "Forget Loader Accuracy: 100.00%\n",
            "Retain Loader Accuracy: 100.00%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the original_model \n",
        "evaluate_instance_model_accuracy(original_model, test_loader, forget_loader, retain_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def distill_with_soft_relabel(original_model, student_model, forget_loader, optimizer, forget_class_idxs, epochs=1, device='cuda', distill_temperature=4):\n",
        "    original_model.to(device)\n",
        "    student_model.to(device)\n",
        "    epoch_losses = []  # Record loss for each epoch\n",
        "    epoch_times = []   # Record training time for each epoch\n",
        "    \n",
        "    if not isinstance(forget_class_idxs, list):\n",
        "        forget_class_idxs = [forget_class_idxs]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()  # Start timing the training (excluding evaluation)\n",
        "        student_model.train()\n",
        "        batch_losses = []\n",
        "\n",
        "        for batch_idx, (data, targets) in enumerate(forget_loader):\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = original_model(data)\n",
        "                for forget_class_idx in forget_class_idxs:\n",
        "                    teacher_logits[:, forget_class_idx] = float('-inf')\n",
        "                soft_labels = F.softmax(teacher_logits / distill_temperature, dim=1)\n",
        "            \n",
        "            student_logits = student_model(data)\n",
        "            student_log_probs = F.log_softmax(student_logits / distill_temperature, dim=1)\n",
        "            loss = F.kl_div(student_log_probs, soft_labels.detach(), reduction='batchmean')\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_losses.append(loss.item())\n",
        "\n",
        "        epoch_loss = sum(batch_losses) / len(batch_losses)\n",
        "        epoch_losses.append(epoch_loss)\n",
        "        \n",
        "        end_time = time.time()  # End timing the training\n",
        "        epoch_training_time = end_time - start_time\n",
        "        epoch_times.append(epoch_training_time)\n",
        "\n",
        "        # Evaluate model on forget_loader, not included in the training time\n",
        "        student_model.eval()\n",
        "        accuracy = evaluate(student_model, forget_loader, device)\n",
        "        student_model.train()\n",
        "        # print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss}, Training Time: {epoch_training_time:.2f} seconds, Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "        # Early stopping if accuracy reaches 0\n",
        "        if accuracy == 0:\n",
        "            # print(f\"Stopping early at epoch {epoch + 1} due to zero accuracy on forget_loader.\")\n",
        "            break\n",
        "\n",
        "    total_training_time = sum(epoch_times)\n",
        "    # print(f\"Total training time excluding evaluation: {total_training_time:.2f} seconds\")\n",
        "\n",
        "    return student_model, epoch_losses, total_training_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loader Accuracy: 82.52%\n",
            "Forget Loader Accuracy: 0.00%\n",
            "Retain Loader Accuracy: 98.20%\n"
          ]
        }
      ],
      "source": [
        "# Initialize the unlearning model\n",
        "SU_model = create_timm_model().to(device)\n",
        "SU_model.load_state_dict(original_model.state_dict())\n",
        "optimizer = torch.optim.Adam(SU_model.parameters(),lr=0.0001)\n",
        "SU_model, distillation_losses, total_training_time = distill_with_soft_relabel(original_model, SU_model, forget_loader, optimizer, forget_class_idxs=forget_class_idxs, epochs=10, device='cuda')\n",
        "evaluate_instance_model_accuracy(SU_model, test_loader, forget_loader, retain_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SULI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_entropy(logits):\n",
        "    probabilities = F.softmax(logits, dim=1)\n",
        "    log_probabilities = F.log_softmax(logits, dim=1)\n",
        "    entropy = -(probabilities * log_probabilities).sum(dim=1)\n",
        "    return entropy\n",
        "\n",
        "def sort_data_loader_by_entropy(teacher_model, data_loader, device, batch_size_per_loader=200):\n",
        "    data_with_entropy = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, targets in data_loader:\n",
        "            data = data.to(device)\n",
        "            logits = teacher_model(data)\n",
        "            entropy = calculate_entropy(logits)\n",
        "            for i in range(len(data)):\n",
        "                data_with_entropy.append((data[i].cpu(), targets[i], entropy[i].item()))\n",
        "\n",
        "    # sort samples by entropy\n",
        "    data_with_entropy.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    # slip subloaders\n",
        "    loaders = []\n",
        "    for i in range(0, len(data_with_entropy), batch_size_per_loader):\n",
        "        batch = data_with_entropy[i:i+batch_size_per_loader]\n",
        "        dataset = [(x[0], x[1]) for x in batch]  \n",
        "        # create new DataLoader\n",
        "        loader = DataLoader(dataset, batch_size=len(batch), shuffle=True)\n",
        "        loaders.append(loader)\n",
        "\n",
        "    return loaders\n",
        "\n",
        "def SPA_Iteration_unlearning(teacher_model, student_model, forget_loader, optimizer, forget_class_idxs, epochs=10, device='cuda', distill_temperature=1):\n",
        "    teacher_model.to(device)\n",
        "    student_model.to(device)\n",
        "    epoch_losses = [] \n",
        "\n",
        "    if not isinstance(forget_class_idxs, list):\n",
        "        forget_class_idxs = [forget_class_idxs]\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        student_model.train()\n",
        "        batch_losses = []\n",
        "\n",
        "        for batch_idx, (data, targets) in enumerate(forget_loader):\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = teacher_model(data)\n",
        "                for forget_class_idx in forget_class_idxs:\n",
        "                    teacher_logits[:, forget_class_idx] = float('-inf')\n",
        "                soft_labels = F.softmax(teacher_logits / distill_temperature, dim=1)\n",
        "            \n",
        "            student_logits = student_model(data)\n",
        "            student_log_probs = F.log_softmax(student_logits / distill_temperature, dim=1)\n",
        "            loss = F.kl_div(student_log_probs, soft_labels.detach(), reduction='batchmean')\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            batch_losses.append(loss.item())\n",
        "\n",
        "        epoch_loss = sum(batch_losses) / len(batch_losses)\n",
        "        epoch_losses.append(epoch_loss)\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss}')\n",
        "\n",
        "    return student_model, epoch_losses\n",
        "\n",
        "def SelfUnlearning_Layered_Iteration(teacher_model, sorted_loaders, forget_class_idxs, forget_loader, epochs=10, device='cuda', distill_temperature=1, lr=0.01):\n",
        "    current_teacher_model = copy.deepcopy(teacher_model).to(device)\n",
        "    previous_accuracy = -1  # Initialize with an impossible value to ensure no rollback on the first run\n",
        "    total_loaders = len(sorted_loaders)\n",
        "    total_time_elapsed = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for loader_index, loader in enumerate(sorted_loaders):\n",
        "            # Save the current state of the model before updates\n",
        "            state_before_update = copy.deepcopy(current_teacher_model.state_dict())\n",
        "            optimizer = torch.optim.Adam(current_teacher_model.parameters(), lr=lr, weight_decay=0)\n",
        "\n",
        "            start = time.time()\n",
        "            current_teacher_model, epoch_losses = SPA_Iteration_unlearning(current_teacher_model, current_teacher_model, loader, optimizer, forget_class_idxs, epochs=1, device=device, distill_temperature=distill_temperature)\n",
        "            end = time.time()\n",
        "            time_elapsed = end - start\n",
        "            total_time_elapsed += time_elapsed\n",
        "\n",
        "            forget_test_accuracy = evaluate(current_teacher_model, forget_loader, device=device)\n",
        "            if forget_test_accuracy > previous_accuracy and previous_accuracy != -1:\n",
        "                # Rollback to the previous state if current accuracy is higher than previous\n",
        "                current_teacher_model.load_state_dict(state_before_update)\n",
        "            else:\n",
        "                # Update previous accuracy and accumulate loss if no rollback\n",
        "                previous_accuracy = forget_test_accuracy\n",
        "                \n",
        "\n",
        "            if forget_test_accuracy == 0:\n",
        "                # print(\"Early stopping triggered due to zero forget accuracy.\")\n",
        "                break\n",
        "        else:\n",
        "            continue\n",
        "        break\n",
        "\n",
        "    # print(f\"Total time elapsed over all epochs and loaders: {total_time_elapsed} seconds\")\n",
        "    return current_teacher_model, total_time_elapsed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, Loss: 0.1719745546579361\n",
            "Epoch 1/1, Loss: 0.04842371121048927\n",
            "Epoch 1/1, Loss: 0.014049886725842953\n",
            "Epoch 1/1, Loss: 0.009383846074342728\n",
            "Epoch 1/1, Loss: 0.0067747230641543865\n",
            "Epoch 1/1, Loss: 0.004645343869924545\n",
            "Epoch 1/1, Loss: 0.004092388320714235\n",
            "Epoch 1/1, Loss: 0.0034357081167399883\n",
            "Epoch 1/1, Loss: 0.0027459857519716024\n",
            "Epoch 1/1, Loss: 0.002673782641068101\n",
            "Epoch 1/1, Loss: 0.0016505284002050757\n",
            "Test Loader Accuracy: 83.14%\n",
            "Forget Loader Accuracy: 0.00%\n",
            "Retain Loader Accuracy: 98.86%\n"
          ]
        }
      ],
      "source": [
        "# Initialize the unlearning model\n",
        "SULI_model = create_timm_model().to(device)\n",
        "SULI_model.load_state_dict(original_model.state_dict())\n",
        "# Sort the samples\n",
        "sorted_loader = sort_data_loader_by_entropy(original_model, forget_loader, device, batch_size_per_loader=500)\n",
        "SULI_model, total_time_elapsed = SelfUnlearning_Layered_Iteration(SULI_model, sorted_loaders=sorted_loader,forget_class_idxs=forget_class_idxs, forget_loader=forget_loader, epochs=10 ,device='cuda',lr=0.0001)\n",
        "evaluate_instance_model_accuracy(SULI_model, test_loader, forget_loader, retain_loader, device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
